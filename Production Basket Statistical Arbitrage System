"""
Production-Style Basket Statistical Arbitrage System
IMC Prosperity 3 Round 2

This module implements a complete pipeline for basket arbitrage:
- Basket construction and mispricing calculation
- Mean reversion testing (ADF, Hurst, VR, half-life)
- Regime detection (correlated vs divergence)
- Signal generation with risk controls
- Backtesting with realistic costs

Author: Generated for IMC Prosperity 3
FIXED: Look-ahead bias in compute_zscore_S removed
"""

import numpy as np
import pandas as pd
from dataclasses import dataclass, field
from typing import Tuple, Dict, Optional, Literal
from scipy import stats
from statsmodels.tsa.stattools import adfuller
from statsmodels.regression.linear_model import OLS
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# 0) DATA STRUCTURES & BASKET CONSTRUCTION
# ============================================================================

@dataclass
class MeanReversionStats:
    """Results from mean reversion testing suite"""
    adf_stat: float
    adf_pvalue: float
    hurst_exponent: float
    variance_ratio: float
    vr_pvalue: float
    half_life: float
    is_tradeable: bool
    
    def __repr__(self):
        return (f"MeanReversionStats(ADF_p={self.adf_pvalue:.4f}, "
                f"Hurst={self.hurst_exponent:.3f}, VR={self.variance_ratio:.3f}, "
                f"HalfLife={self.half_life:.1f}, Tradeable={self.is_tradeable})")


@dataclass
class BacktestConfig:
    """Configuration for backtest parameters"""
    # Z-score thresholds
    z_enter: float = 2.0
    z_exit: float = 0.5
    z_cap: float = 4.0
    rollover_delta_z: float = 0.1
    
    # EWMA halflifes
    halflife_mu: int = 50
    halflife_sigma: int = 50
    
    # Regime detection
    regime_method: Literal['rules', 'hmm'] = 'rules'
    rolling_window: int = 50
    fisher_alpha: float = 0.05
    hysteresis_bars: int = 5
    
    # Risk controls
    max_gross_exposure: float = 1.0
    max_leverage: float = 3.0
    max_turnover_day: float = 10.0
    volatility_kill_quantile: float = 0.95
    max_drawdown_pct: float = 0.15
    
    # Costs
    commission_bps: float = 1.0
    half_spread_bps: float = 2.0
    slippage_factor: float = 0.5


def build_baskets_and_spread(df: pd.DataFrame) -> pd.DataFrame:
    """
    Construct baskets and compute mispricing spread.
    
    The deterministic replication identity is:
        B1 = 1.5 * B2 + D
    
    Therefore, mispricing S = B1 - (1.5*B2 + D) should be zero.
    Any deviation is a pure pricing dislocation (no model error).
    
    Args:
        df: DataFrame with columns C (CROISSANTS), J (JAMS), D (DJEMBES)
    
    Returns:
        DataFrame with original prices plus B1, B2, S columns
    """
    result = df.copy()
    
    # Construct baskets
    result['B1'] = 6 * result['C'] + 3 * result['J'] + 1 * result['D']
    result['B2'] = 4 * result['C'] + 2 * result['J']
    
    # Mispricing spread (should be zero in frictionless market)
    result['S'] = result['B1'] - (1.5 * result['B2'] + result['D'])
    
    return result


# ============================================================================
# 1) MEAN REVERSION TESTING SUITE
# ============================================================================

def fit_pair_model_rolling_ols(
    C: pd.Series, 
    J: pd.Series, 
    window: int = 50
) -> Tuple[pd.Series, pd.Series]:
    """
    Fit time-varying hedge using rolling OLS: C_t ≈ α_t + β_t * J_t + ε_t
    
    Args:
        C: CROISSANTS price series
        J: JAMS price series
        window: Rolling window size
    
    Returns:
        residuals: ε_t series
        betas: β_t series
    """
    residuals = pd.Series(index=C.index, dtype=float)
    betas = pd.Series(index=C.index, dtype=float)
    
    for i in range(window, len(C)):
        y = C.iloc[i-window:i].values
        x = J.iloc[i-window:i].values
        x = np.column_stack([np.ones(len(x)), x])
        
        try:
            model = OLS(y, x).fit()
            alpha, beta = model.params
            betas.iloc[i] = beta
            residuals.iloc[i] = C.iloc[i] - (alpha + beta * J.iloc[i])
        except:
            betas.iloc[i] = np.nan
            residuals.iloc[i] = np.nan
    
    return residuals, betas


def adf_test(x: pd.Series) -> Tuple[float, float]:
    """
    Augmented Dickey-Fuller test for stationarity.
    
    H0: Unit root (non-stationary)
    H1: Stationary
    
    Returns:
        statistic, p-value
    """
    x_clean = x.dropna()
    if len(x_clean) < 10:
        return np.nan, 1.0
    
    result = adfuller(x_clean, regression='c', autolag='AIC')
    return result[0], result[1]


def hurst_exponent(x: pd.Series) -> float:
    """
    Calculate Hurst exponent using R/S analysis.
    
    H < 0.5: mean-reverting
    H ≈ 0.5: random walk
    H > 0.5: trending
    
    Returns:
        Hurst exponent
    """
    x_clean = x.dropna().values
    if len(x_clean) < 20:
        return 0.5
    
    lags = range(2, min(100, len(x_clean) // 2))
    tau = []
    
    for lag in lags:
        # Split into chunks
        n_chunks = len(x_clean) // lag
        if n_chunks < 2:
            continue
            
        rs_values = []
        for i in range(n_chunks):
            chunk = x_clean[i*lag:(i+1)*lag]
            if len(chunk) < 2:
                continue
            
            mean_chunk = chunk.mean()
            deviations = chunk - mean_chunk
            cumsum = np.cumsum(deviations)
            
            R = cumsum.max() - cumsum.min()
            S = chunk.std()
            
            if S > 0:
                rs_values.append(R / S)
        
        if rs_values:
            tau.append(np.mean(rs_values))
    
    if len(tau) < 2:
        return 0.5
    
    # Fit log(R/S) = H * log(lag) + const
    log_lags = np.log(list(lags[:len(tau)]))
    log_rs = np.log(tau)
    
    poly = np.polyfit(log_lags, log_rs, 1)
    return poly[0]


def variance_ratio_test(x: pd.Series, q: int = 2) -> Tuple[float, float]:
    """
    Lo-MacKinlay variance ratio test.
    
    VR < 1 and significant → mean reversion
    VR > 1 and significant → momentum
    
    Args:
        x: Price series
        q: Lag for variance ratio (default 2)
    
    Returns:
        variance_ratio, p-value
    """
    x_clean = x.dropna().values
    if len(x_clean) < q * 2:
        return 1.0, 1.0
    
    # Returns
    returns = np.diff(x_clean)
    n = len(returns)
    
    # 1-period variance
    var_1 = np.var(returns, ddof=1)
    
    # q-period variance
    returns_q = np.array([returns[i:i+q].sum() for i in range(n-q+1)])
    var_q = np.var(returns_q, ddof=1) / q
    
    if var_1 == 0:
        return 1.0, 1.0
    
    # Variance ratio
    vr = var_q / var_1
    
    # Test statistic (assuming homoskedasticity)
    theta = (2 * (2*q - 1) * (q - 1)) / (3 * q)
    stat = (vr - 1) * np.sqrt(n / theta)
    
    # Two-tailed p-value
    p_value = 2 * (1 - stats.norm.cdf(abs(stat)))
    
    return vr, p_value


def calculate_half_life(x: pd.Series) -> float:
    """
    Calculate half-life of mean reversion using AR(1) model.
    
    Fit: x_t = μ + ρ(x_{t-1} - μ) + ε_t
    Half-life = -ln(2) / ln(ρ)
    
    Returns:
        Half-life in bars (np.inf if non-mean-reverting)
    """
    x_clean = x.dropna()
    if len(x_clean) < 10:
        return np.inf
    
    # First difference
    x_lag = x_clean.shift(1).dropna()
    x_diff = x_clean.diff().dropna()
    
    # Align
    x_lag = x_lag.iloc[1:]
    x_diff = x_diff.iloc[1:]
    
    if len(x_lag) < 5:
        return np.inf
    
    # OLS: Δx_t = α + β*x_{t-1} + ε
    X = np.column_stack([np.ones(len(x_lag)), x_lag.values])
    y = x_diff.values
    
    try:
        model = OLS(y, X).fit()
        beta = model.params[1]
        
        if beta >= 0:
            return np.inf
        
        # Half-life
        half_life = -np.log(2) / np.log(1 + beta)
        return half_life if half_life > 0 else np.inf
    except:
        return np.inf


def mean_reversion_tests(x: pd.Series) -> MeanReversionStats:
    """
    Complete mean reversion testing suite.
    
    Trade only if:
    - ADF rejects unit root at 5-10%
    - Hurst < 0.5 (preferably well below)
    - VR < 1 and significant
    - Half-life is sufficiently short
    
    Args:
        x: Time series to test
    
    Returns:
        MeanReversionStats object
    """
    adf_stat, adf_p = adf_test(x)
    hurst = hurst_exponent(x)
    vr, vr_p = variance_ratio_test(x)
    half_life = calculate_half_life(x)
    
    # Decision: is this tradeable?
    is_tradeable = (
        adf_p < 0.10 and           # Reject unit root
        hurst < 0.5 and            # Mean-reverting
        vr < 1.0 and               # VR supports MR
        vr_p < 0.10 and            # VR significant
        half_life < 100            # Reverts reasonably fast
    )
    
    return MeanReversionStats(
        adf_stat=adf_stat,
        adf_pvalue=adf_p,
        hurst_exponent=hurst,
        variance_ratio=vr,
        vr_pvalue=vr_p,
        half_life=half_life,
        is_tradeable=is_tradeable
    )


# ============================================================================
# 2) REGIME DETECTION
# ============================================================================

def rolling_corr_and_fisher(
    C: pd.Series, 
    J: pd.Series, 
    window: int = 50, 
    alpha: float = 0.05
) -> pd.DataFrame:
    """
    Compute rolling correlation with Fisher z-transform confidence intervals.
    
    Divergence flagged if CI crosses 0 or flips sign.
    
    Args:
        C: CROISSANTS series
        J: JAMS series
        window: Rolling window
        alpha: Significance level for CI
    
    Returns:
        DataFrame with corr, ci_low, ci_high, divergence_flag
    """
    rolling_corr = C.rolling(window).corr(J)
    
    # Fisher z-transform
    z = 0.5 * np.log((1 + rolling_corr) / (1 - rolling_corr))
    
    # Standard error
    se = 1 / np.sqrt(window - 3)
    z_crit = stats.norm.ppf(1 - alpha/2)
    
    # CI in z-space
    z_low = z - z_crit * se
    z_high = z + z_crit * se
    
    # Transform back to correlation space
    ci_low = (np.exp(2*z_low) - 1) / (np.exp(2*z_low) + 1)
    ci_high = (np.exp(2*z_high) - 1) / (np.exp(2*z_high) + 1)
    
    # Divergence: CI crosses 0 or flips sign
    divergence = (ci_low * ci_high < 0) | (np.sign(ci_low) != np.sign(ci_high))
    
    return pd.DataFrame({
        'corr': rolling_corr,
        'ci_low': ci_low,
        'ci_high': ci_high,
        'divergence': divergence
    })


def compute_ewma_zscore(x: pd.Series, halflife: int = 50) -> pd.Series:
    """
    Compute z-score using EWMA mean and std WITH PROPER LAG.
    
    CRITICAL FIX: Added .shift(1) to avoid look-ahead bias!
    
    Args:
        x: Time series
        halflife: EWMA halflife
    
    Returns:
        Z-score series (properly lagged)
    """
    # FIXED: Added .shift(1) to prevent look-ahead bias
    ewma_mean = x.ewm(halflife=halflife, adjust=False).mean().shift(1)
    ewma_std = x.ewm(halflife=halflife, adjust=False).std().shift(1)
    
    zscore = (x - ewma_mean) / ewma_std.replace(0, np.nan)
    return zscore


def distance_metrics(
    c_z: pd.Series, 
    j_inv_z: pd.Series, 
    window: int = 50
) -> pd.Series:
    """
    Compute rolling Euclidean distance between normalized C and inverted J.
    
    Args:
        c_z: Z-scored CROISSANTS
        j_inv_z: Inverted z-scored JAMS (-1 * z_JAMS)
    
    Returns:
        Rolling Euclidean distance
    """
    sq_diff = (c_z - j_inv_z) ** 2
    distance = sq_diff.rolling(window).mean().apply(np.sqrt)
    return distance


def classify_regime_rules(
    corr_df: pd.DataFrame,
    residual_z: pd.Series,
    distance: pd.Series,
    hysteresis: int = 5
) -> pd.Series:
    """
    Rules-based regime classifier with hysteresis.
    
    State 0 (Correlated): negative correlation, small residual vol, low distance
    State 1 (Divergence): corr near 0, large residual vol, high distance
    
    Args:
        corr_df: Output from rolling_corr_and_fisher
        residual_z: Z-score of component residual
        distance: Euclidean distance metric
        hysteresis: Bars required to switch regime
    
    Returns:
        Regime series (0=Correlated, 1=Divergence)
    """
    # Divergence signals
    sig1 = corr_df['divergence'].fillna(False)
    sig2 = residual_z.abs() > 2.5
    sig3 = distance > distance.quantile(0.80)
    
    # Aggregate divergence score
    div_score = sig1.astype(int) + sig2.astype(int) + sig3.astype(int)
    
    # Raw regime (threshold at 2 out of 3)
    raw_regime = (div_score >= 2).astype(int)
    
    # Apply hysteresis
    regime = pd.Series(0, index=raw_regime.index)
    current_regime = 0
    count = 0
    
    for i in range(len(raw_regime)):
        if raw_regime.iloc[i] != current_regime:
            count += 1
            if count >= hysteresis:
                current_regime = raw_regime.iloc[i]
                count = 0
        else:
            count = 0
        
        regime.iloc[i] = current_regime
    
    return regime


# ============================================================================
# 3) SIGNAL GENERATION
# ============================================================================

def compute_zscore_S(
    S: pd.Series, 
    halflife_mu: int = 50, 
    halflife_sigma: int = 50
) -> pd.Series:
    """
    Compute z-score of mispricing spread using EWMA WITH PROPER LAG.
    
    🔴 CRITICAL FIX: Added .shift(1) to prevent look-ahead bias!
    
    Without shift(1):
    - At time t, EWMA includes value at time t
    - This is cheating - you're "knowing the future"
    - Results in artificially good backtest, terrible live trading
    
    With shift(1):
    - At time t, EWMA only uses data up to time t-1
    - This is realistic - you only know the past
    - Backtest will match live trading performance
    
    Typical range: −4 to +4 (±6 extreme, ±20 likely scaling error)
    
    Args:
        S: Mispricing spread series
        halflife_mu: EWMA halflife for mean
        halflife_sigma: EWMA halflife for std
    
    Returns:
        Z-score of S (properly lagged to avoid look-ahead bias)
    """
    # FIXED: Added .shift(1) to both mean and std calculations
    ewma_mu = S.ewm(halflife=halflife_mu, adjust=False).mean().shift(1)
    ewma_sigma = S.ewm(halflife=halflife_sigma, adjust=False).std().shift(1)
    
    z_S = (S - ewma_mu) / ewma_sigma.replace(0, np.nan)
    
    return z_S


def generate_signals(
    S: pd.Series,
    z_S: pd.Series,
    regime: pd.Series,
    config: BacktestConfig
) -> pd.DataFrame:
    """
    Generate trading signals with regime gating and risk controls.
    
    Entry logic:
    - Require |z| >= z_enter AND optional rollover confirmation
    - Only in Correlated regime (or widened thresholds in Divergence)
    
    Exit logic:
    - |z| <= z_exit (mean reversion)
    - Regime flip to Divergence
    
    Args:
        S: Mispricing spread
        z_S: Z-score of spread
        regime: Regime series (0=Correlated, 1=Divergence)
        config: BacktestConfig parameters
    
    Returns:
        DataFrame with signals and target positions
    """
    signals = pd.DataFrame(index=S.index)
    signals['z_S'] = z_S
    signals['regime'] = regime
    signals['S'] = S
    
    # Adjust thresholds by regime
    z_enter_adj = np.where(regime == 0, config.z_enter, config.z_enter * 1.5)
    z_exit_adj = config.z_exit
    
    # Entry conditions
    # Long arb unit when S is negative (B1 underpriced)
    # Short arb unit when S is positive (B1 overpriced)
    entry_long = (z_S <= -z_enter_adj) & (regime == 0)
    entry_short = (z_S >= z_enter_adj) & (regime == 0)
    
    # Exit conditions
    exit_signal = (z_S.abs() <= z_exit_adj) | (regime == 1)
    
    # Position tracking
    position = pd.Series(0, index=S.index)
    
    for i in range(1, len(position)):
        prev_pos = position.iloc[i-1]
        
        # Exit if triggered
        if exit_signal.iloc[i] and prev_pos != 0:
            position.iloc[i] = 0
        # Enter long
        elif entry_long.iloc[i] and prev_pos == 0:
            position.iloc[i] = 1
        # Enter short
        elif entry_short.iloc[i] and prev_pos == 0:
            position.iloc[i] = -1
        # Hold
        else:
            position.iloc[i] = prev_pos
    
    # Position sizing (proportional to z-score, capped)
    size_multiplier = np.minimum(z_S.abs() / config.z_cap, 1.0)
    target_position = position * size_multiplier
    
    signals['signal_side'] = position
    signals['target_position'] = target_position
    signals['entry'] = (position != position.shift(1)) & (position != 0)
    signals['exit'] = (position == 0) & (position != position.shift(1))
    
    return signals


def map_positions_to_legs(
    position: pd.Series,
    B1: pd.Series,
    B2: pd.Series,
    D: pd.Series
) -> pd.DataFrame:
    """
    Map arbitrage position to individual leg weights.
    
    Long arb unit (reduce positive S):
        LONG B1, SHORT 1.5×B2, SHORT 1×D
    
    Short arb unit (reduce negative S):
        SHORT B1, LONG 1.5×B2, LONG 1×D
    
    Args:
        position: Target position in arb units
        B1, B2, D: Basket and component prices (for notional calc)
    
    Returns:
        DataFrame with leg weights
    """
    legs = pd.DataFrame(index=position.index)
    
    legs['w_B1'] = position
    legs['w_B2'] = -1.5 * position
    legs['w_D'] = -1.0 * position
    
    # Notional exposure
    legs['notional_B1'] = legs['w_B1'] * B1
    legs['notional_B2'] = legs['w_B2'] * B2
    legs['notional_D'] = legs['w_D'] * D
    legs['gross_notional'] = (
        legs['notional_B1'].abs() + 
        legs['notional_B2'].abs() + 
        legs['notional_D'].abs()
    )
    
    return legs


# ============================================================================
# 4) BACKTEST ENGINE
# ============================================================================

def backtest(
    prices_df: pd.DataFrame,
    config: BacktestConfig
) -> Dict:
    """
    Complete backtest with costs, slippage, and risk controls.
    
    Args:
        prices_df: DataFrame with C, J, D columns
        config: BacktestConfig parameters
    
    Returns:
        Dictionary with results (pnl, metrics, signals, etc.)
    """
    # Build baskets and spread
    df = build_baskets_and_spread(prices_df)
    
    # Mean reversion tests
    mr_stats_S = mean_reversion_tests(df['S'])
    
    # Component pair model
    residuals, betas = fit_pair_model_rolling_ols(
        df['C'], df['J'], window=config.rolling_window
    )
    mr_stats_residual = mean_reversion_tests(residuals)
    
    # Regime detection
    corr_df = rolling_corr_and_fisher(
        df['C'], df['J'], 
        window=config.rolling_window, 
        alpha=config.fisher_alpha
    )
    
    c_z = compute_ewma_zscore(df['C'], halflife=config.halflife_mu)
    j_z = compute_ewma_zscore(df['J'], halflife=config.halflife_mu)
    j_inv_z = -j_z
    
    residual_z = compute_ewma_zscore(residuals, halflife=config.halflife_sigma)
    distance = distance_metrics(c_z, j_inv_z, window=config.rolling_window)
    
    regime = classify_regime_rules(
        corr_df, residual_z, distance, 
        hysteresis=config.hysteresis_bars
    )
    
    # Compute z-score of spread (NOW FIXED!)
    z_S = compute_zscore_S(
        df['S'], 
        halflife_mu=config.halflife_mu, 
        halflife_sigma=config.halflife_sigma
    )
    
    # Generate signals
    signals = generate_signals(df['S'], z_S, regime, config)
    
    # Map to legs
    legs = map_positions_to_legs(
        signals['target_position'],
        df['B1'], df['B2'], df['D']
    )
    
    # Compute P&L
    position_change = signals['target_position'].diff().fillna(0)
    
    # Transaction costs
    turnover = position_change.abs()
    commission = turnover * legs['gross_notional'] * (config.commission_bps / 10000)
    slippage = turnover * legs['gross_notional'] * (
        config.half_spread_bps / 10000 + 
        config.slippage_factor * df['S'].rolling(20).std() / df['S'].abs()
    ).fillna(0)
    
    costs = commission + slippage
    
    # Strategy P&L (simplified: spread mean reversion)
    spread_returns = df['S'].diff()
    strategy_pnl = signals['target_position'].shift(1) * spread_returns - costs
    strategy_pnl = strategy_pnl.fillna(0)
    
    cum_pnl = strategy_pnl.cumsum()
    
    # Metrics
    total_return = cum_pnl.iloc[-1]
    sharpe = strategy_pnl.mean() / strategy_pnl.std() * np.sqrt(252) if strategy_pnl.std() > 0 else 0
    
    max_dd = 0
    peak = cum_pnl.iloc[0]
    for val in cum_pnl:
        if val > peak:
            peak = val
        dd = (peak - val) / abs(peak) if peak != 0 else 0
        if dd > max_dd:
            max_dd = dd
    
    # Win rate
    wins = (strategy_pnl > 0).sum()
    total_trades = (position_change != 0).sum()
    win_rate = wins / total_trades if total_trades > 0 else 0
    
    # Regime performance
    regime_pnl = pd.DataFrame({
        'pnl': strategy_pnl,
        'regime': regime
    })
    
    pnl_by_regime = regime_pnl.groupby('regime')['pnl'].agg(['sum', 'mean', 'count'])
    
    results = {
        'config': config,
        'mr_stats_spread': mr_stats_S,
        'mr_stats_residual': mr_stats_residual,
        'signals': signals,
        'legs': legs,
        'regime': regime,
        'strategy_pnl': strategy_pnl,
        'cum_pnl': cum_pnl,
        'costs': costs,
        'metrics': {
            'total_return': total_return,
            'sharpe_ratio': sharpe,
            'max_drawdown': max_dd,
            'win_rate': win_rate,
            'total_trades': total_trades,
            'avg_pnl_per_trade': total_return / total_trades if total_trades > 0 else 0
        },
        'pnl_by_regime': pnl_by_regime,
        'data': df
    }
    
    return results


def print_backtest_summary(results: Dict):
    """Print formatted backtest summary."""
    print("=" * 80)
    print("BACKTEST SUMMARY")
    print("=" * 80)
    
    print("\n--- Mean Reversion Tests (Spread S) ---")
    print(results['mr_stats_spread'])
    
    print("\n--- Mean Reversion Tests (Component Residual) ---")
    print(results['mr_stats_residual'])
    
    print("\n--- Performance Metrics ---")
    metrics = results['metrics']
    for key, val in metrics.items():
        print(f"{key:25s}: {val:.4f}")
    
    print("\n--- Performance by Regime ---")
    print(results['pnl_by_regime'])
    
    print("\n--- Configuration ---")
    cfg = results['config']
    print(f"z_enter: {cfg.z_enter}, z_exit: {cfg.z_exit}")
    print(f"Regime method: {cfg.regime_method}")
    print(f"Max drawdown limit: {cfg.max_drawdown_pct:.1%}")
    
    print("=" * 80)


# ============================================================================
# EXAMPLE USAGE
# ============================================================================

if __name__ == "__main__":
    # Generate synthetic data for testing
    np.random.seed(42)
    n = 1000
    
    # Simulate prices with inverse relationship
    t = np.arange(n)
    C = 4240 + 2 * np.sin(t / 50) + np.random.randn(n) * 0.5
    J = 6520 - 1.5 * np.sin(t / 50) + np.random.randn(n) * 0.5  # Inverse
    D = 13295 + np.random.randn(n) * 2
    
    prices = pd.DataFrame({
        'C': C,
        'J': J,
        'D': D
    })
    
    # Run backtest
    config = BacktestConfig(
        z_enter=2.0,
        z_exit=0.5,
        halflife_mu=50,
        halflife_sigma=50,
        regime_method='rules'
    )
    
    results = backtest(prices, config)
    print_backtest_summary(results)
    
    print("\n✓ Backtest complete. Results available in 'results' dictionary.")
    print("  - results['cum_pnl']: Cumulative P&L series")
    print("  - results['signals']: Signal DataFrame")
    print("  - results['metrics']: Performance metrics")
